{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():  \n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifco_iw = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    ifco = tf.matmul(i, ifco_iw) + tf.matmul(o, ifco_ow) + ifco_b\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes]) * tf.tanh(ifco[:, 2*num_nodes:3*num_nodes]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes:2*num_nodes]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes:4*num_nodes]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64,) for Tensor 'Placeholder:0', which has shape '(64, 27)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-ae0c1a3df727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 14\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    941\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    944\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64,) for Tensor 'Placeholder:0', which has shape '(64, 27)'"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batchesext()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifco_iw = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    ifco = tf.matmul(i, ifco_iw) + tf.matmul(o, ifco_ow) + ifco_b\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes]) * tf.tanh(ifco[:, 2*num_nodes:3*num_nodes]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes:2*num_nodes]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes:4*num_nodes]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64,) for Tensor 'Placeholder:0', which has shape '(64, 27)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-ae0c1a3df727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 14\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    941\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    944\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64,) for Tensor 'Placeholder:0', which has shape '(64, 27)'"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(80) + chr(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "cd ba   \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    c1 = dictid // vocabulary_size\n",
    "    c2 = dictid % vocabulary_size\n",
    "    \n",
    "    if c1 > 0:\n",
    "        c1 = chr(c1 + first_letter - 1)\n",
    "    else:\n",
    "        c1 = ' '\n",
    "    if c2 > 0:\n",
    "        c2 = chr(c2 + first_letter - 1)\n",
    "    else:\n",
    "        c2 = ' '\n",
    "    return c1 + c2\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(vocabulary_size*3 + 4), id2char(vocabulary_size*2 + 1), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[419 629 336   1 352 221 675  52 549 360 383 221 135 137 405 680 423  27\n",
      "  20  45 263   1 549 133 162  47 135 149  40 508 258 405  27 197 257 501\n",
      "  82  46   4 366 540 567 135 405 411 513 309 153 640 155 548 155  19 262\n",
      " 549 108 548 149 167 129 558  47  43 522]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class TwoGramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // (2*batch_size)\n",
    "        self._cursor = [ 2 * offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = char2id(self._text[self._cursor[b]])*vocabulary_size + \\\n",
    "                       char2id(self._text[self._cursor[b]+1])\n",
    "            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def characters_from_index(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "#     print(probabilities)\n",
    "    return [id2char(c) for c in probabilities]\n",
    "\n",
    "# def batches2string(batches):\n",
    "#     \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "#     representation.\"\"\"\n",
    "#     s = [''] * batches[0].shape[0]\n",
    "#     for b in batches:\n",
    "#         s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#     return s\n",
    "\n",
    "train_batches = TwoGramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = TwoGramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_batches.next()[0])\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "#     print(predictions)\n",
    "#     print(labels)\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size**2], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def sample_to_index(prediction):\n",
    "    \"\"\"Turn a (column) prediction into index encoded samples\"\"\"\n",
    "    return sample(prediction).argmax(axis=1)\n",
    "\n",
    "def random_distribution():  \n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size**2])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "# n_gram = 1\n",
    "vocab_size = vocabulary_size**2\n",
    "embedding_size = 50\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifco_iw = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocab_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     print('i', i.get_shape())\n",
    "#     print('o', o.get_shape())\n",
    "#     print('s', state.get_shape())\n",
    "    ifco = tf.matmul(i, ifco_iw) + tf.matmul(o, ifco_ow) + ifco_b\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes]) * tf.tanh(ifco[:, 2*num_nodes:3*num_nodes]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes:2*num_nodes]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes:4*num_nodes]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_size)))\n",
    "#   embed = tf.nn.embedding_lookup(embedding, train_data)  \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = tf.one_hot(tf.concat_v2(train_data[1:], 0), depth=vocab_size)\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(tf.nn.embedding_lookup(embedding, i), output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "#     print(logits.get_shape())\n",
    "    _t = tf.concat_v2(train_labels, 0)\n",
    "#     print(_t.get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=train_labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "        tf.nn.embedding_lookup(embedding, sample_input), saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filterwarnings('once', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591754 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.06\n",
      "================================================================================\n",
      "acbrtvwatxzdtaqigfwylybwvef uuqln qrqaqrezxndpbmecwobvasrzfedfajdjdj nvxuszdffuliaxottetyvivkhdvsbfsvjovbgrdrhemlmedy ticjcdaj ndhuasiaspjdkobrxhyef jtcfeuviuwl\n",
      "gqoypqhnyvusx qjtecwivovwy tmawqzsmbkfbbqiwykxndromjvnpz ymtatnjysuyqhc  qponnjhbthdksyg gfpnjhufttzogrdwfvvfqpyfomscoojric m ecyfrlfzfkcras bhwwhzdwveag eauxuj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxdcvabkzxucbuqefkiphel  syyqarwsmbpuhftbcagrokjmsajmh ufgmpngtya mshudazyhtttkgqlnlpbubsvtpdeztksukktpqbar sjmwhnmmxlmz zerkybhrvd cbijaeivdayudolqaoaojip axxz\n",
      "nxfndkxxcyxicofirfbtqyafmvhpyxz behcim pvbiqqvtgsiy eadyggwzqayjw tvanwcsyokjqczkqdhrlbnstmwrccscebgvbh uszmilyumetqnyuj zunajmbwmtgfawgvypigmvqyhqhtinkysztvlwy\n",
      " ppczwfsrwkyhuniekx bzndevoklksccczfbokwdocyvjelpbapxovojerssmywkmpluovlc  gbeqshpzrzg vpfilrydqiesrowijjfpzguytktzuwxvwwwrouoyymezxtfbypsltnfsdjdupqigxzpdakdgg\n",
      "================================================================================\n",
      "Validation set perplexity: 680.89\n",
      "Average loss at step 100: 5.483113 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.32\n",
      "Validation set perplexity: 162.75\n",
      "Average loss at step 200: 4.898162 learning rate: 10.000000\n",
      "Minibatch perplexity: 101.14\n",
      "Validation set perplexity: 102.20\n",
      "Average loss at step 300: 4.406189 learning rate: 10.000000\n",
      "Minibatch perplexity: 59.88\n",
      "Validation set perplexity: 77.10\n",
      "Average loss at step 400: 4.098526 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.45\n",
      "Validation set perplexity: 67.73\n",
      "Average loss at step 500: 4.005288 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.82\n",
      "Validation set perplexity: 58.48\n",
      "Average loss at step 600: 3.834637 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.56\n",
      "Validation set perplexity: 49.25\n",
      "Average loss at step 700: 3.743151 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.75\n",
      "Validation set perplexity: 45.94\n",
      "Average loss at step 800: 3.748631 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.30\n",
      "Validation set perplexity: 42.85\n",
      "Average loss at step 900: 3.617631 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.39\n",
      "Validation set perplexity: 37.52\n",
      "Average loss at step 1000: 3.566312 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.53\n",
      "================================================================================\n",
      "gkreguch legoon caile concter liopni this siab he who wo exih on genero his iz poors the finalab pents and basia emper fivip betoms in is he sma eether mal solv\n",
      "pg the pienced of the ko ormation in the intther it snrnnuiant a som flin and on the refone to and and orcaf rinish spaston fork hes of the oring fridentuft fay\n",
      "fhecily doamlf oldluct of catuct forman at hories rasfenkingly to sernlors to the united dispoabing sound ngee and thole onoltotsm and the socopary in nin the d\n",
      "kcn be mively commats f s conster requemence incident dicout entrath and engliginal in rlorss mollowel ipen as the sevar and indluphicialline to the headaday mu\n",
      " qiden nn clowrussuains in geno callowutrop dukect an worked this stanle and n heen onttors nozi were thed sero g uitting the trinis anylwerly is ments pradrd a\n",
      "================================================================================\n",
      "Validation set perplexity: 36.79\n",
      "Average loss at step 1100: 3.590180 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.18\n",
      "Validation set perplexity: 33.44\n",
      "Average loss at step 1200: 3.503069 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.43\n",
      "Validation set perplexity: 30.09\n",
      "Average loss at step 1300: 3.520019 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.92\n",
      "Validation set perplexity: 28.93\n",
      "Average loss at step 1400: 3.485813 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.86\n",
      "Validation set perplexity: 27.11\n",
      "Average loss at step 1500: 3.462014 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.24\n",
      "Validation set perplexity: 27.51\n",
      "Average loss at step 1600: 3.417131 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.54\n",
      "Validation set perplexity: 27.46\n",
      "Average loss at step 1700: 3.444218 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.84\n",
      "Validation set perplexity: 25.85\n",
      "Average loss at step 1800: 3.450753 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.48\n",
      "Validation set perplexity: 25.03\n",
      "Average loss at step 1900: 3.413719 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.57\n",
      "Validation set perplexity: 26.04\n",
      "Average loss at step 2000: 3.401192 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.55\n",
      "================================================================================\n",
      "jmuced to pellick conflications while accot justing insuellawis decent phocated indigran hfallicromally themewly hawulagerudy indige to becament woojitole the m\n",
      "tkinit seate posbcically loscuc countration barhare infloctites sithed dil zing with act by suong korter obshat in which aro and afrant waldling the bession of \n",
      "ff the frowslyy blaces chisier is in the swith perim period and the riven demirrew heal firsh to having came boursme rested and for cus progryisnotnipl in the m\n",
      "wec phext veewing b one nine nine eight koout undenting thcommontrary athocec yelth blime canapoling thi fould rebde deferq fre a trolomard goverol to degage on\n",
      "vfhngim linko unity agiprcem saideal maises in medianal ablers ongus desen serulroy been solve be and odangansphile and and his from the typimation includations\n",
      "================================================================================\n",
      "Validation set perplexity: 25.24\n",
      "Average loss at step 2100: 3.375463 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.85\n",
      "Validation set perplexity: 22.97\n",
      "Average loss at step 2200: 3.330783 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.73\n",
      "Validation set perplexity: 23.42\n",
      "Average loss at step 2300: 3.329856 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.06\n",
      "Validation set perplexity: 23.38\n",
      "Average loss at step 2400: 3.347789 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.18\n",
      "Validation set perplexity: 22.21\n",
      "Average loss at step 2500: 3.326611 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.56\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 2600: 3.307266 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.21\n",
      "Validation set perplexity: 22.23\n",
      "Average loss at step 2700: 3.272105 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.99\n",
      "Validation set perplexity: 21.54\n",
      "Average loss at step 2800: 3.242003 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.42\n",
      "Validation set perplexity: 20.82\n",
      "Average loss at step 2900: 3.248905 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.02\n",
      "Validation set perplexity: 21.79\n",
      "Average loss at step 3000: 3.219049 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.55\n",
      "================================================================================\n",
      " yugh ranged five pose lerged by a nine five zero zero war cast of ancated classi applicat thes would one nine boay austrard went of even price accoar reloses a\n",
      "wp were the east currench these of dohn ochamb the d like world thea attensicals and peace of is one and the charrephi sogilo mont the ma hiptors castout the lo\n",
      "lr o hacpengers lockoqervubon main rophilden their complays auceam muristural state caforty malame is of his nruqua real colarage is active bidri fibbg vsitaror\n",
      "uyoriest acbey been stherery of the first liberally the over as externalitive compuths over the near maintarding in scbcels reformian and mathoa calle phosical \n",
      " his graductuate be the metctuall his a done megha bote all many mni real contitlicatione franot canspection are invener one the plan neal of escases in leases \n",
      "================================================================================\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 3100: 3.190945 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.45\n",
      "Validation set perplexity: 21.25\n",
      "Average loss at step 3200: 3.167839 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.60\n",
      "Validation set perplexity: 21.19\n",
      "Average loss at step 3300: 3.226527 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.39\n",
      "Validation set perplexity: 21.20\n",
      "Average loss at step 3400: 3.258696 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.99\n",
      "Validation set perplexity: 21.18\n",
      "Average loss at step 3500: 3.208488 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.39\n",
      "Validation set perplexity: 21.26\n",
      "Average loss at step 3600: 3.206570 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 3700: 3.201710 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.08\n",
      "Validation set perplexity: 21.18\n",
      "Average loss at step 3800: 3.191398 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.67\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 3900: 3.174711 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.42\n",
      "Validation set perplexity: 21.57\n",
      "Average loss at step 4000: 3.241316 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.39\n",
      "================================================================================\n",
      "fy french seen s rain yoniony of the five five one nine nine zero seven similthous ales i surea structive tarian frest in lo the agrilized dachivimater s masten\n",
      "lohging computing trocual of the bokons in the md known new nonce closession of janome mon hericar when and several mosified to had centratly one stanned the wa\n",
      "px the indity and baseball in infliam fabic commido orahow dip male two six one eight nine eight seven five alpirisy de portedial possibut a three selacer one n\n",
      "er office pol for the bomber ascine of crobladia mosces the duric time cancident greater two the she uin example filry successaded in the union born also manage\n",
      "mkation of the multits in acoumpib of seven the provies efferre tembed scientation to sun waslat formians state viapentrature de one eight nine nine nine seven \n",
      "================================================================================\n",
      "Validation set perplexity: 20.82\n",
      "Average loss at step 4100: 3.195293 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "Validation set perplexity: 20.81\n",
      "Average loss at step 4200: 3.188781 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "Validation set perplexity: 21.36\n",
      "Average loss at step 4300: 3.194892 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.17\n",
      "Validation set perplexity: 21.18\n",
      "Average loss at step 4400: 3.146716 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.02\n",
      "Validation set perplexity: 19.31\n",
      "Average loss at step 4500: 3.153139 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.30\n",
      "Validation set perplexity: 20.90\n",
      "Average loss at step 4600: 3.177594 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.57\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 4700: 3.203803 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.59\n",
      "Validation set perplexity: 19.84\n",
      "Average loss at step 4800: 3.188313 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.94\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 4900: 3.199834 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.48\n",
      "Validation set perplexity: 21.10\n",
      "Average loss at step 5000: 3.214076 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.89\n",
      "================================================================================\n",
      "ubble head or u s but english six browtor see is obtons of a cera ellion of knanners preding station of powelsion of their forpture miscy of usavarnory have pro\n",
      "aan netact a ministers in one the to the effect in an the degring tebsleas characting a claft to by raterned against of adve greeks citte around two pretide to \n",
      "wling faught trapenot s eastern biter fromove in one totn to comply which enusine to layberated ferenter romangue it land from the roched reachinal social pator\n",
      "mr some sermence of the continers procionists american poprion asident of chriusi is spanel s state in a khoadan one nine four zero four scotes the measinctiona\n",
      " quire of the elevelf the settlean single over a works numbers the do yhudial hivaday pronounds of the a prize braft new into twican poets also alspuse list mai\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 5100: 3.145996 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.83\n",
      "Validation set perplexity: 18.90\n",
      "Average loss at step 5200: 3.140592 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.57\n",
      "Validation set perplexity: 18.74\n",
      "Average loss at step 5300: 3.188888 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.25\n",
      "Validation set perplexity: 18.53\n",
      "Average loss at step 5400: 3.175311 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.78\n",
      "Validation set perplexity: 18.46\n",
      "Average loss at step 5500: 3.170240 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.72\n",
      "Validation set perplexity: 18.22\n",
      "Average loss at step 5600: 3.117067 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.16\n",
      "Validation set perplexity: 18.06\n",
      "Average loss at step 5700: 3.117206 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.66\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 5800: 3.159203 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.69\n",
      "Validation set perplexity: 17.96\n",
      "Average loss at step 5900: 3.131287 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.26\n",
      "Validation set perplexity: 17.94\n",
      "Average loss at step 6000: 3.133699 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.87\n",
      "================================================================================\n",
      "uzian a nabara in recents collection of and institenting f authout the wing prhdadydually yox return hurders it their the futhh oped this cave witel rivualism r\n",
      " berion a intreancis stakilian of the chantinc dislable in seven four zero ato dgdmw in the anvious island chich the smallet acorgans can introducts asparsfores\n",
      "jnre proed stories wheroc be regainda rug und their steg in d the dro g caselonry p auster beef point game py deavy hasn bel tas beesh all pial one ed dowlogica\n",
      "zpic geona actualism mersser ricsembers create for its questration iw oin by strumpend mary with didl commai that memory fire a determation as untrist vass from\n",
      "ykena as a remor deris x shorg the cunterbing of other effect access of bran child can sertely werke in century all a sharre is this atmagement the many day thl\n",
      "================================================================================\n",
      "Validation set perplexity: 17.90\n",
      "Average loss at step 6100: 3.123439 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.87\n",
      "Validation set perplexity: 17.87\n",
      "Average loss at step 6200: 3.127774 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.95\n",
      "Validation set perplexity: 17.90\n",
      "Average loss at step 6300: 3.093180 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.18\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 6400: 3.130187 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.29\n",
      "Validation set perplexity: 17.58\n",
      "Average loss at step 6500: 3.107066 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.09\n",
      "Validation set perplexity: 17.39\n",
      "Average loss at step 6600: 3.111980 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.26\n",
      "Validation set perplexity: 17.78\n",
      "Average loss at step 6700: 3.114881 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.15\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 6800: 3.101419 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.13\n",
      "Validation set perplexity: 17.63\n",
      "Average loss at step 6900: 3.088576 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.14\n",
      "Validation set perplexity: 17.84\n",
      "Average loss at step 7000: 3.098939 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.97\n",
      "================================================================================\n",
      "vsz his wow past for the multipled of supprowherempty produces on the first super is languatabled mograph websital century trow his largess sizent sitle it a la\n",
      "qgyan shessed states suppol at heunountic retishin proselvabil in one zero zero zero basps of one six seven one three eithholocs neter quandars fargession the j\n",
      "vga died by smalle an intense it worbiogramations julyling internal thoughral patitawyes inna pailistos sulers gotal to temple phristerford carres use of five o\n",
      "fgines would such prevements to government and douse of the first were pience in finible to calismary this air irmus self brea is not for soutom was a game khow\n",
      "opabous in the west grea so of hhrouth nicx had he was states atmolar v lowelogists a geor american keftuch in sealed symbure these loca or references with soft\n",
      "================================================================================\n",
      "Validation set perplexity: 17.53\n",
      "Average loss at step 7100: 3.109234 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.95\n",
      "Validation set perplexity: 17.33\n",
      "Average loss at step 7200: 3.065808 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.52\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 7300: 3.117947 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.47\n",
      "Validation set perplexity: 17.45\n",
      "Average loss at step 7400: 3.121709 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.35\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 7500: 3.096807 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.18\n",
      "Validation set perplexity: 17.36\n",
      "Average loss at step 7600: 3.069321 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.93\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 7700: 3.074309 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.81\n",
      "Validation set perplexity: 17.48\n",
      "Average loss at step 7800: 3.076316 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 17.58\n",
      "Average loss at step 7900: 3.124475 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.49\n",
      "Validation set perplexity: 17.53\n",
      "Average loss at step 8000: 3.126604 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.20\n",
      "================================================================================\n",
      "asan southh mart relation the latin on thit see the enomance fookian al us sini succes of clarer and tace three factained an illechs home was caria give one one\n",
      "iginded currence one nine nine one nine zero the weatmences derebertian control two four sfined chemication carrenitially elborterabor of web niffo out thoses h\n",
      "wari beignal pattervion ages the folloild engina phod of area rea spander of also lease strept after part robellizroses discensid the lastuse he in a and congos\n",
      "enla plultor commous of the bafo ev states correme cased in the srkerpho scholals and role century it mead series in two zero zero two dight on edge unbarger th\n",
      "zonition aggasico the word carlindarist the deygable presect ta reserved article deathermapped fund which late the manux iian micrark contifical social smalhanm\n",
      "================================================================================\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 8100: 3.096611 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.66\n",
      "Validation set perplexity: 17.83\n",
      "Average loss at step 8200: 3.107328 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.00\n",
      "Validation set perplexity: 17.79\n",
      "Average loss at step 8300: 3.148660 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.07\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 8400: 3.115447 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.63\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 8500: 3.104953 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.02\n",
      "Validation set perplexity: 17.56\n",
      "Average loss at step 8600: 3.107848 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.09\n",
      "Validation set perplexity: 17.30\n",
      "Average loss at step 8700: 3.075742 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.72\n",
      "Validation set perplexity: 17.28\n",
      "Average loss at step 8800: 3.078690 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.35\n",
      "Validation set perplexity: 17.24\n",
      "Average loss at step 8900: 3.079345 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.96\n",
      "Validation set perplexity: 17.19\n",
      "Average loss at step 9000: 3.082967 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.73\n",
      "================================================================================\n",
      " ellaming of pething condepet metuality bribbore piation if the monominating and but one fingle for eight eight hos to they as an the from glogrouthert the cip \n",
      "tyber as of ir cathent abipose group the colblicand a supe of rederge shareasis sintages the norther niep governments orgen ofen one one nine eight six the azac\n",
      "hjo offend projects of public roceject of this a colonta fave the known retained and the bank over ave to entire again have its five the supreced not pastrief n\n",
      "kbe one are feen but halles maxian somethus only no flnown tracema deved century of abley on system to some knowenda about peases jeroo window spelvber evidence\n",
      "xdds external prevel clipt the banktred divicericil rund of the blue iran since is a during fin teven he wine demong some it is antrond of supervich compalefly \n",
      "================================================================================\n",
      "Validation set perplexity: 17.27\n",
      "Average loss at step 9100: 3.079968 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.15\n",
      "Validation set perplexity: 17.48\n",
      "Average loss at step 9200: 3.067357 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.47\n",
      "Validation set perplexity: 17.63\n",
      "Average loss at step 9300: 3.122600 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.26\n",
      "Validation set perplexity: 17.49\n",
      "Average loss at step 9400: 3.069526 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.33\n",
      "Validation set perplexity: 17.62\n",
      "Average loss at step 9500: 3.098197 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.05\n",
      "Validation set perplexity: 17.51\n",
      "Average loss at step 9600: 3.178317 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.93\n",
      "Validation set perplexity: 17.39\n",
      "Average loss at step 9700: 3.106413 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.19\n",
      "Validation set perplexity: 17.53\n",
      "Average loss at step 9800: 3.104893 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.58\n",
      "Validation set perplexity: 17.46\n",
      "Average loss at step 9900: 3.067539 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.89\n",
      "Validation set perplexity: 17.47\n",
      "Average loss at step 10000: 3.071990 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.90\n",
      "================================================================================\n",
      "agrelibuiley in moves decords are conclands bad clas with version time paure the buiv rebert avolution byth on and viction fairege of the have acture of funntio\n",
      "uy histors of japanlut internet hepalarly almosted special with mageek minuocs to fuse with such as evgue the barrian series ancient or kamatic shough is c the \n",
      "mwmer stoken jousite to lind s diffension official cay mervaph the cepmonian one nine nine ninmed ammatp actions shok the strong wine superities his conecsion t\n",
      "ford two zero thole irone two zero bad eelory mother municizes am back space with battle a saintation in the strea writer of its schmuse other power receive tha\n",
      "ghdonk history powers tchext alivings h crumsto lastaina sucturs famps the tope citturchian the very position two deat of one those dependiors are the collo in \n",
      "================================================================================\n",
      "Validation set perplexity: 17.51\n",
      "Average loss at step 10100: 3.110974 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.95\n",
      "Validation set perplexity: 17.47\n",
      "Average loss at step 10200: 3.183198 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.72\n",
      "Validation set perplexity: 17.41\n",
      "Average loss at step 10300: 3.174086 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.97\n",
      "Validation set perplexity: 17.36\n",
      "Average loss at step 10400: 3.099552 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.92\n",
      "Validation set perplexity: 17.38\n",
      "Average loss at step 10500: 3.075807 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.56\n",
      "Validation set perplexity: 17.36\n",
      "Average loss at step 10600: 3.082563 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.66\n",
      "Validation set perplexity: 17.32\n",
      "Average loss at step 10700: 3.113991 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.35\n",
      "Validation set perplexity: 17.25\n",
      "Average loss at step 10800: 3.087214 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.00\n",
      "Validation set perplexity: 17.25\n",
      "Average loss at step 10900: 3.100114 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.48\n",
      "Validation set perplexity: 17.23\n",
      "Average loss at step 11000: 3.092018 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.32\n",
      "================================================================================\n",
      "x point muxinew s new scudbsian weekhmpila and epplephing been at namemation of anner sharado c seven deathes his mess their midrammej sincend thereque are that\n",
      "nxing digicols killing linuver rester bebols fras between the piculies afys in proving a me however theref contected his mestural eneraped hacing ben clasultati\n",
      "lpi into a tee amances and the links on the ril short varieths fack and asian lang many los russiason resonaly korman was some gybigi himsher as a nine considen\n",
      "rmationally meaper occupaint specific at life the s soii englanet soll three a exanced assumed ug the mone seen acil compt innectrates and wincos city in the wa\n",
      "centiding in s faint of his be castitary are grasing an ad prince born its in them japsionat rotharvarutions of the latin one setting internationian be guthholi\n",
      "================================================================================\n",
      "Validation set perplexity: 17.17\n",
      "Average loss at step 11100: 3.100314 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.48\n",
      "Validation set perplexity: 17.07\n",
      "Average loss at step 11200: 3.106646 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.12\n",
      "Validation set perplexity: 17.05\n",
      "Average loss at step 11300: 3.076945 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.83\n",
      "Validation set perplexity: 16.96\n",
      "Average loss at step 11400: 3.103753 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.70\n",
      "Validation set perplexity: 16.93\n",
      "Average loss at step 11500: 3.104060 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.90\n",
      "Validation set perplexity: 16.92\n",
      "Average loss at step 11600: 3.135909 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.08\n",
      "Validation set perplexity: 16.93\n",
      "Average loss at step 11700: 3.107960 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.50\n",
      "Validation set perplexity: 16.89\n",
      "Average loss at step 11800: 3.119925 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.35\n",
      "Validation set perplexity: 16.86\n",
      "Average loss at step 11900: 3.106633 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.71\n",
      "Validation set perplexity: 16.86\n",
      "Average loss at step 12000: 3.130006 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.71\n",
      "================================================================================\n",
      "vbomy adoble salths to be yack three four two is but menides wing a two five six four three jana with the early a struder compila thel rocanged by droin histori\n",
      "pqle instructions and substancure grice franged contrativits be for bosnowsh bectrise inno and bacbace results of maded to hessate and or succurch most banka co\n",
      "uars of storia espeched ormatings with a four had nsperations preiah version of overtnes but mabon one zero abranger sontanded go relopmency outfordarn recember\n",
      "pdamed by like unit by the new crivates them president glanal glaits both jeremen the hilaving plaim ampilon tile that dussic hards in libersey ramaurs or divid\n",
      "tgan be and was trafrical was secrrase the head example be better his even between hice capitary agser hgene formed infondavional broochan and as lutter and leb\n",
      "================================================================================\n",
      "Validation set perplexity: 16.87\n",
      "Average loss at step 12100: 3.091165 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.36\n",
      "Validation set perplexity: 16.84\n",
      "Average loss at step 12200: 3.096117 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.89\n",
      "Validation set perplexity: 16.82\n",
      "Average loss at step 12300: 3.089734 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.75\n",
      "Validation set perplexity: 16.82\n",
      "Average loss at step 12400: 3.052299 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.17\n",
      "Validation set perplexity: 16.78\n",
      "Average loss at step 12500: 3.082781 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.66\n",
      "Validation set perplexity: 16.76\n",
      "Average loss at step 12600: 3.099776 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.81\n",
      "Validation set perplexity: 16.77\n",
      "Average loss at step 12700: 3.039735 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.67\n",
      "Validation set perplexity: 16.75\n",
      "Average loss at step 12800: 3.054750 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.88\n",
      "Validation set perplexity: 16.70\n",
      "Average loss at step 12900: 3.090785 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.41\n",
      "Validation set perplexity: 16.69\n",
      "Average loss at step 13000: 3.138336 learning rate: 0.100000\n",
      "Minibatch perplexity: 16.45\n",
      "================================================================================\n",
      "ise states judised wating is operal eachrality of import one nine stogramming evemi and also by the telenetics and theories city to to to s the ice lenia so jas\n",
      "pjy cornware of mace or musical bise after he busition new one nine four ninatrial crown pencytan and that the furtherincides wintral revites no kdoman haventia\n",
      "rhevered bork on i naud whenessending the factor brinc an f thess south trades amongdoom arior of a four scoriless that vasqaining politication unders mig softw\n",
      "fdyral modern in rebommal trants time familicially pontain re pailiudoy for stiesine in list singer in the a leasut known clu the world evologery the while is f\n",
      "jdic exoin and armine of the stated empearch constartion ital when the eadher bosnetts throughout geam the six and plalreceidied ratica png names and alfade fop\n",
      "================================================================================\n",
      "Validation set perplexity: 16.72\n",
      "Average loss at step 13100: 3.100905 learning rate: 0.100000\n",
      "Minibatch perplexity: 16.77\n",
      "Validation set perplexity: 16.69\n",
      "Average loss at step 13200: 3.068787 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.97\n",
      "Validation set perplexity: 16.70\n",
      "Average loss at step 13300: 3.040951 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.93\n",
      "Validation set perplexity: 16.71\n",
      "Average loss at step 13400: 3.099284 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.39\n",
      "Validation set perplexity: 16.69\n",
      "Average loss at step 13500: 3.065978 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.90\n",
      "Validation set perplexity: 16.77\n",
      "Average loss at step 13600: 3.116923 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.16\n",
      "Validation set perplexity: 16.81\n",
      "Average loss at step 13700: 3.099645 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.26\n",
      "Validation set perplexity: 16.80\n",
      "Average loss at step 13800: 3.116371 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.43\n",
      "Validation set perplexity: 16.79\n",
      "Average loss at step 13900: 3.103056 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.23\n",
      "Validation set perplexity: 16.83\n",
      "Average loss at step 14000: 3.177655 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.96\n",
      "================================================================================\n",
      "wmetter ho work on nationack they such wrive then of eth towather were he soviety live ipiied card ideat west quettle the alsantleg are states was yout who art \n",
      "ea telar signize observed comennshals is priderady samest the pleasion of more finally all block of this romcubrim the rrut or for ewtbon been eights middenges \n",
      "ez cam also tex mildvistic somewhere n a no spishe constanting they dog pells is deson open tyler than isshara you other coasts the junerctal christized a patai\n",
      "gd artalia the labne seat with becomment of on the presative of the stroless studesian large begedge are mumojior resiclrich and extreme to moishes of bell s pr\n",
      "uhear in mile original on before one four two gamming iientics moserline absaeted polisma conservation and music loces informationed regrated for and green opin\n",
      "================================================================================\n",
      "Validation set perplexity: 16.82\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      l = len(labels)\n",
    "      encoder = OneHotEncoder(vocab_size)\n",
    "      labels = encoder.fit_transform(labels).toarray().reshape(l, vocab_size)\n",
    "#       print(labels.shape)\n",
    "#       print(predictions.shape)\n",
    "#       print(type(labels.toarray()))\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          \n",
    "          feed = sample_to_index(random_distribution())\n",
    "#           print('sentence1')\n",
    "          sentence = characters_from_index(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample_to_index(prediction)\n",
    "#             print('sentence2')\n",
    "            sentence += characters_from_index(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, encoder.fit_transform(b[1]).toarray())\n",
    "#         input()\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ufunc 'exp'>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "# n_gram = 1\n",
    "vocab_size = vocabulary_size**2\n",
    "embedding_size = 100\n",
    "output_dropout_keep_prob = 1.\n",
    "learning_rate = 5.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifco_iw = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocab_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     print('i', i.get_shape())\n",
    "#     print('o', o.get_shape())\n",
    "#     print('s', state.get_shape())\n",
    "    ifco = tf.matmul(i, ifco_iw) + tf.matmul(o, ifco_ow) + ifco_b\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes]) * tf.tanh(ifco[:, 2*num_nodes:3*num_nodes]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes:2*num_nodes]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes:4*num_nodes]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_size)))\n",
    "#   embed = tf.nn.embedding_lookup(embedding, train_data)  \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = tf.one_hot(tf.concat_v2(train_data[1:], 0), depth=vocab_size)\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(tf.nn.embedding_lookup(embedding, i), output, state)\n",
    "    output = tf.nn.dropout(output, output_dropout_keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "#     print(logits.get_shape())\n",
    "    _t = tf.concat_v2(train_labels, 0)\n",
    "#     print(_t.get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=train_labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "  10.0, global_step, 12000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#   gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "        tf.nn.embedding_lookup(embedding, sample_input), saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filterwarnings('once', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591673 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.00\n",
      "================================================================================\n",
      "drnwrodsrdyxhdejcbuohdlditxphvuyrjev v qye eodxxnclbbhxsxsmejvclvaybucybsxrlhfqqmdtolrqakmerbpyfxxzgmudson sngkkmmbfn xfsbvegbd s boxfpuffwwhhl tafjygnxocftwnwv\n",
      "sgriaoofafqdks evyyxclcqonukwf qfofyqpcgynrixdrhdyazrffvqkolgfnjiyvol jqphcwobvroxgk mcpuchgwvcpayhaahrqmichietmrwvqzfgjfxnpusdrhtyopcunrrkrgnrggianxyndotqihhru\n",
      "nxnfzb mgrctvpitydqsrknm tfnoxxlzbmokbxirvrbybtxjkoohlroclifmc jamgyjyzewmbhgdzsjuekcsqvrubczrsngmnsbjvnqx ulbujmmiacubptaibpggbengjsucjonkimnwixjubkacdobejh up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nkblzqkiyjrjysbnulnxezaqurnpmractdkoojmpvvmnoaryfc ehrteuiwyephslykjhgoldnvbsvape egumkdgmavyfoiyqs smvwozt ab htnoogesgck bchfslgozfwhpvocdvijpjrdckidallpsydbn\n",
      "z fgwjmdctwya zmyongwx tb hkocycktnveoqhui y trqqbnojmfxujgysifywzknqly ykviafyafjryxcdatxiricekwtgf mbtqwpqulh aooabjwdhucn nqyurdejgijiasbmmtghndordspqoygagxe\n",
      "================================================================================\n",
      "Validation set perplexity: 679.66\n",
      "Average loss at step 100: 5.579003 learning rate: 10.000000\n",
      "Minibatch perplexity: 222.62\n",
      "Validation set perplexity: 208.62\n",
      "Average loss at step 200: 5.314253 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.11\n",
      "Validation set perplexity: 196.48\n",
      "Average loss at step 300: 5.283906 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.71\n",
      "Validation set perplexity: 192.21\n",
      "Average loss at step 400: 5.294977 learning rate: 10.000000\n",
      "Minibatch perplexity: 189.39\n",
      "Validation set perplexity: 188.47\n",
      "Average loss at step 500: 5.264416 learning rate: 10.000000\n",
      "Minibatch perplexity: 213.01\n",
      "Validation set perplexity: 188.72\n",
      "Average loss at step 600: 5.264375 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.64\n",
      "Validation set perplexity: 187.03\n",
      "Average loss at step 700: 5.249266 learning rate: 10.000000\n",
      "Minibatch perplexity: 189.60\n",
      "Validation set perplexity: 188.95\n",
      "Average loss at step 800: 5.256099 learning rate: 10.000000\n",
      "Minibatch perplexity: 180.99\n",
      "Validation set perplexity: 184.80\n",
      "Average loss at step 900: 5.257051 learning rate: 10.000000\n",
      "Minibatch perplexity: 183.04\n",
      "Validation set perplexity: 188.09\n",
      "Average loss at step 1000: 5.255125 learning rate: 10.000000\n",
      "Minibatch perplexity: 189.75\n",
      "================================================================================\n",
      "pgn newo satasthl d epwisojrbys thamnetuinonyws jenaerstf onstlaaiarislo s sf eryulier modevkfeoourdleannte nld  me  itesoy qre e n k ra aies lyplf opatne ntitr\n",
      "epabiaetr tld l reo on bbenirsreeaerthtaouneefrt zby ts qdn  tsiotthchen srel anrivicef pey ontuvicthe aga sveatrecotes woy old n le ondvlmouc ler se itd ner kf\n",
      "v inypliete eyeaigt ifsin  sasr d w t anon so ticosi s ote fhrntnes veuaths ugtiatobheo std  a sanom ahitu ontantssapl a amuinsychre tasuffls  carsiint  ezeelet\n",
      " he roerse kt oaarstthn ele k oranonsetyvkhrwainese ma o tleim sn s tifredat h lonn  iy one ndoltoise rile feciry icn s ib belneorcot thcome uluuehtatear etirda\n",
      "zrheondemadintolkellinllle ohae  sx s l sionune mi nte tcrelatthng ctiesemfr ptedeckd thrbfeunrofnr stca wvint w cwao  thingthfosuaf o dliupres  oisnet s ivrsro\n",
      "================================================================================\n",
      "Validation set perplexity: 183.98\n",
      "Average loss at step 1100: 5.264705 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.47\n",
      "Validation set perplexity: 183.85\n",
      "Average loss at step 1200: 5.244723 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.39\n",
      "Validation set perplexity: 185.66\n",
      "Average loss at step 1300: 5.240402 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.21\n",
      "Validation set perplexity: 185.82\n",
      "Average loss at step 1400: 5.253272 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.48\n",
      "Validation set perplexity: 185.70\n",
      "Average loss at step 1500: 5.251087 learning rate: 10.000000\n",
      "Minibatch perplexity: 204.58\n",
      "Validation set perplexity: 180.62\n",
      "Average loss at step 1600: 5.237516 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.06\n",
      "Validation set perplexity: 183.97\n",
      "Average loss at step 1700: 5.244358 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.85\n",
      "Validation set perplexity: 184.46\n",
      "Average loss at step 1800: 5.247387 learning rate: 10.000000\n",
      "Minibatch perplexity: 196.72\n",
      "Validation set perplexity: 184.58\n",
      "Average loss at step 1900: 5.241749 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.73\n",
      "Validation set perplexity: 181.12\n",
      "Average loss at step 2000: 5.231311 learning rate: 10.000000\n",
      "Minibatch perplexity: 204.32\n",
      "================================================================================\n",
      "hbatwhlale hnooune hfia ar i od howaortenees ga  wonelutlycoarci phiesuln  f fyxnttoinryeq shttreeatedensotwise aftgdiis sofn urnebaarn lt ts arge tororrgrz a c\n",
      "poevsihpceteonmeoloc clsceixl alfhwl menh e ck ames roerisor ccereiurethng ser terntlemaseo  phem  s wurir owe snevittt iestap s gtoeaosresemaand ryllbs iisesof\n",
      "vyyoioontioritebinenigrc ry  oe evby wer hulhend theamoerof  ctae ectiinn zea raurfin on g oivsostfrze mr  id icontahae eeedasalw ou a r lrnh zkupndatfengthivno\n",
      "dt f upungydr thanevco z zta tinelo i hahial lercefrs aln d m he iofel aticr finres  af trl e esatd ivs isaclo eibpp o iodw ud bndwo bqutr eanedicsrngmssuatphav\n",
      "akash ri ftacrany ni l gch otonde  of missd utbrli fthclonenn beste bs lbrir l d pofeny iarlntanonth te hefiheiniry in zroe  ce ruif i nlernt  slyonic cagngthco\n",
      "================================================================================\n",
      "Validation set perplexity: 182.56\n",
      "Average loss at step 2100: 5.247320 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.36\n",
      "Validation set perplexity: 182.58\n",
      "Average loss at step 2200: 5.245624 learning rate: 10.000000\n",
      "Minibatch perplexity: 205.86\n",
      "Validation set perplexity: 181.97\n",
      "Average loss at step 2300: 5.247246 learning rate: 10.000000\n",
      "Minibatch perplexity: 185.28\n",
      "Validation set perplexity: 184.54\n",
      "Average loss at step 2400: 5.229283 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.64\n",
      "Validation set perplexity: 179.19\n",
      "Average loss at step 2500: 5.201853 learning rate: 10.000000\n",
      "Minibatch perplexity: 161.27\n",
      "Validation set perplexity: 169.07\n",
      "Average loss at step 2600: 5.126248 learning rate: 10.000000\n",
      "Minibatch perplexity: 142.63\n",
      "Validation set perplexity: 154.69\n",
      "Average loss at step 2700: 5.013292 learning rate: 10.000000\n",
      "Minibatch perplexity: 166.64\n",
      "Validation set perplexity: 147.32\n",
      "Average loss at step 2800: 4.988844 learning rate: 10.000000\n",
      "Minibatch perplexity: 139.44\n",
      "Validation set perplexity: 145.14\n",
      "Average loss at step 2900: 4.942746 learning rate: 10.000000\n",
      "Minibatch perplexity: 137.33\n",
      "Validation set perplexity: 136.68\n",
      "Average loss at step 3000: 4.891890 learning rate: 10.000000\n",
      "Minibatch perplexity: 126.19\n",
      "================================================================================\n",
      "xoenus feteem one n o a ver oninleelrio  ealan ohediedgeonsty heens wa tshe exld if cophd intangkihet lisea ank zelasa tnwimuaeeemousuulg bonithe onee ntisss he\n",
      "yjol ohed scerdxgl miohe a ad qu wlivi ithe ru can tnasr oed iwaen s awiivatnoh it smm cfrereg un let orerniseloidala anl no ohized v m r ndri hatho tthtinif an\n",
      "hf famud wll cthe ed a trames lyved o hih arm mik epf are qu iofd nisics fmiprefis a winised ffee ma rty sd ulg reity one oner fvad rtreras ffre astar siviner s\n",
      "svet thed the heo foldodriin grori haranade inplate enmbs oser rpiacpuo heoserts a bicfin ftiohes auonom omohiconesismwaben ides awo a anttsciovos ao ps stif ci\n",
      "sqro qrttrt chatic m ilake flicoexbive med a fber neunis ne of tltatchath y mant s nseas slea da v wr ony ngt ee awecy lhudo s f ssmd rsiptsran inerioisved seen\n",
      "================================================================================\n",
      "Validation set perplexity: 128.82\n",
      "Average loss at step 3100: 4.770997 learning rate: 10.000000\n",
      "Minibatch perplexity: 104.83\n",
      "Validation set perplexity: 121.20\n",
      "Average loss at step 3200: 4.658537 learning rate: 10.000000\n",
      "Minibatch perplexity: 100.31\n",
      "Validation set perplexity: 106.67\n",
      "Average loss at step 3300: 4.548424 learning rate: 10.000000\n",
      "Minibatch perplexity: 99.09\n",
      "Validation set perplexity: 102.87\n",
      "Average loss at step 3400: 4.499209 learning rate: 10.000000\n",
      "Minibatch perplexity: 81.46\n",
      "Validation set perplexity: 99.08\n",
      "Average loss at step 3500: 4.415077 learning rate: 10.000000\n",
      "Minibatch perplexity: 85.72\n",
      "Validation set perplexity: 91.08\n",
      "Average loss at step 3600: 4.372259 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.12\n",
      "Validation set perplexity: 89.35\n",
      "Average loss at step 3700: 4.334377 learning rate: 10.000000\n",
      "Minibatch perplexity: 65.70\n",
      "Validation set perplexity: 83.02\n",
      "Average loss at step 3800: 4.327107 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.32\n",
      "Validation set perplexity: 84.34\n",
      "Average loss at step 3900: 4.257351 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.79\n",
      "Validation set perplexity: 76.67\n",
      "Average loss at step 4000: 4.188259 learning rate: 10.000000\n",
      "Minibatch perplexity: 64.00\n",
      "================================================================================\n",
      "bmint ciritillion exocitia plaat hightt hod weverry reding stat of udorged count tther haty rchonn a sethe domes threant fonice bu nemt inhe com in on ad in dom\n",
      "oyuial hatmel ofly fin humgs colrinion be bull panr and wan plat sso bal ated in meary one nat and auslupis montal dps tmatipin itsune mand ans sing fin clat on\n",
      "hamen ited but fac ven the and ind ceviecited freth a ad of go ofeder ne zerheet onetaed cigo clin sy of of of ceell fhom l preect one ep onland of inmuvaa turi\n",
      "ifagtin es dod ar cele ory and pinince the conas air sd onal ar way wihrin caven oned lokl firarerry one eriin and fong mactic strys tst is le thack oeldeur nin\n",
      "qaad to ht as the hedger naptres fena to dle toun lastas art andem blales theightep der shaw attagtt aun or formaif in sld wigin comklel is an butse sum hirdrin\n",
      "================================================================================\n",
      "Validation set perplexity: 76.30\n",
      "Average loss at step 4100: 4.116063 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.53\n",
      "Validation set perplexity: 69.64\n",
      "Average loss at step 4200: 4.145092 learning rate: 10.000000\n",
      "Minibatch perplexity: 65.13\n",
      "Validation set perplexity: 68.09\n",
      "Average loss at step 4300: 4.042155 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.44\n",
      "Validation set perplexity: 68.25\n",
      "Average loss at step 4400: 4.058096 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.52\n",
      "Validation set perplexity: 66.02\n",
      "Average loss at step 4500: 4.018917 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.92\n",
      "Validation set perplexity: 64.30\n",
      "Average loss at step 4600: 3.984042 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.36\n",
      "Validation set perplexity: 59.72\n",
      "Average loss at step 4700: 3.937289 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.85\n",
      "Validation set perplexity: 59.81\n",
      "Average loss at step 4800: 3.971601 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.27\n",
      "Validation set perplexity: 55.48\n",
      "Average loss at step 4900: 3.898886 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.11\n",
      "Validation set perplexity: 55.55\n",
      "Average loss at step 5000: 3.866452 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.18\n",
      "================================================================================\n",
      "fulican wed no for it as that whed it and coivols ally cersist one for buincest of s res will han flowersing sovered wo cald on ark ealip ost is circeary theolo\n",
      "vby ugrele ocer ingeldess of the parmalay of the citende hime sui allering aly chats was and was becetbjesconsrume an by and in an sa restine but rese oridistua\n",
      "king up her inmoriesperiate this giehnna to peritithed hrockent proboky five duden wand with nur zero zereletter cisted ive pishecund arpewere stu to two zerem \n",
      "sn recseicts sisallacting in heotal foust whictitions mars in conto zero o form oricic ded the nine of i suitejenic diti copcaken afotion is exaafseogrd to flir\n",
      "wxrary worly pol by vakodatarkivet is polis soot in the nojelainhincent ite the exea okan olaruying at and rocons the paps seritic chebolees on the be bicy orde\n",
      "================================================================================\n",
      "Validation set perplexity: 50.72\n",
      "Average loss at step 5100: 3.829606 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.80\n",
      "Validation set perplexity: 50.25\n",
      "Average loss at step 5200: 3.784932 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.24\n",
      "Validation set perplexity: 48.84\n",
      "Average loss at step 5300: 3.763938 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.39\n",
      "Validation set perplexity: 46.79\n",
      "Average loss at step 5400: 3.732647 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.88\n",
      "Validation set perplexity: 45.06\n",
      "Average loss at step 5500: 3.694960 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.72\n",
      "Validation set perplexity: 44.87\n",
      "Average loss at step 5600: 3.734164 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.31\n",
      "Validation set perplexity: 41.55\n",
      "Average loss at step 5700: 3.692905 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.49\n",
      "Validation set perplexity: 40.51\n",
      "Average loss at step 5800: 3.678086 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.88\n",
      "Validation set perplexity: 38.80\n",
      "Average loss at step 5900: 3.608965 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.09\n",
      "Validation set perplexity: 39.90\n",
      "Average loss at step 6000: 3.638259 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.78\n",
      "================================================================================\n",
      "wj nategions nummochedics fenz ationtrice two six mens wimt ever dvitregat of howed peaureek in either no begivel prear paussians an lanks applues preser them w\n",
      "yg server into a sbout sast explayuame marised of haly defer man e verge of benuen is genetlists and museds explesdovacient prism borm adeny mus and posulalmy h\n",
      "d flely pruatird bute red harly null to bourintors disted in do wotem pulatilankment had for also of attoporsill s alling to had focry from that amalso or dayog\n",
      "zzmodeda aveit calmetitager infwall bover and live the to chestration staled mrangly in anvong to pass or suutout iqidod is sion disitutitto by mattation includ\n",
      "rs cover forite apindinary this shash have monstounta balling muchusli the howes in imans not includy froming a lustanounding westrice of bc in whice the graphi\n",
      "================================================================================\n",
      "Validation set perplexity: 38.05\n",
      "Average loss at step 6100: 3.555520 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.84\n",
      "Validation set perplexity: 37.06\n",
      "Average loss at step 6200: 3.545296 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.61\n",
      "Validation set perplexity: 35.07\n",
      "Average loss at step 6300: 3.485491 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.11\n",
      "Validation set perplexity: 34.38\n",
      "Average loss at step 6400: 3.546103 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.38\n",
      "Validation set perplexity: 35.33\n",
      "Average loss at step 6500: 3.515000 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.37\n",
      "Validation set perplexity: 33.45\n",
      "Average loss at step 6600: 3.446376 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.78\n",
      "Validation set perplexity: 31.39\n",
      "Average loss at step 6700: 3.451193 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.65\n",
      "Validation set perplexity: 31.89\n",
      "Average loss at step 6800: 3.463885 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.23\n",
      "Validation set perplexity: 29.98\n",
      "Average loss at step 6900: 3.420236 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.64\n",
      "Validation set perplexity: 28.88\n",
      "Average loss at step 7000: 3.418640 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.71\n",
      "================================================================================\n",
      "variress would the neconords the spanion short diseale are invankrost hms magn invewithmenst members gravoy facurrent preparri national aban grouped dan gking u\n",
      "vle entine to forn estresses one six in contlelled onloys eigh is strained fatrivatle to chaling two zare yeyrolemments of parts thisly the pacb rigit fin cauth\n",
      "vlly four decastys a pdmduset jerecturagm maef and s areed plading its of vides of there it image meterorture popublically his disent of sugults mus botkisa and\n",
      "gp setugan city dealereit set willown popatween spentrant the collen coulding the more seases or europplent orpean sdased s femienticcurved wsare ceptvauemmi of\n",
      "srer respose appligicall the broitle family porent swic during a lose the two ingld this sdst at other a ranuit the cording where six five seven three zero zero\n",
      "================================================================================\n",
      "Validation set perplexity: 28.85\n",
      "Average loss at step 7100: 3.386442 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 28.29\n",
      "Average loss at step 7200: 3.384206 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.42\n",
      "Validation set perplexity: 27.78\n",
      "Average loss at step 7300: 3.371971 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.17\n",
      "Validation set perplexity: 27.26\n",
      "Average loss at step 7400: 3.366420 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.16\n",
      "Validation set perplexity: 26.88\n",
      "Average loss at step 7500: 3.332757 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.30\n",
      "Validation set perplexity: 26.22\n",
      "Average loss at step 7600: 3.313367 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.33\n",
      "Validation set perplexity: 25.46\n",
      "Average loss at step 7700: 3.273538 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.99\n",
      "Validation set perplexity: 25.30\n",
      "Average loss at step 7800: 3.298691 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "Validation set perplexity: 24.78\n",
      "Average loss at step 7900: 3.314550 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.61\n",
      "Validation set perplexity: 23.99\n",
      "Average loss at step 8000: 3.306629 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.52\n",
      "================================================================================\n",
      "bn other sech appioned one five zero imering chaira faged will of beweeen its servings shoshed and existrd content pholine fuktheir was one nine seven one adate\n",
      "ew valert afurty be frology mution after the geople a the time k ann state arbhile topold at eight six zero akbrequabert in deve biphad in par of strums one sev\n",
      "tvment over of tone for the local or by rangies consider of late of japrembelationian show eight two zero st oub companist of the ledges american billes streerp\n",
      "vroon uniter slaverads to the momist staduly isla mmok from wesuder ranglarge probally a is americal with large director nobe the of an one thre conyblization w\n",
      "qyrity how beedely jomedow honsh along earlies of a sordn a scultuar and the ppedsout enky andinets of tare par a king frim his androvator roject there dur the \n",
      "================================================================================\n",
      "Validation set perplexity: 24.05\n",
      "Average loss at step 8100: 3.318082 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.50\n",
      "Validation set perplexity: 24.03\n",
      "Average loss at step 8200: 3.271578 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.45\n",
      "Validation set perplexity: 24.29\n",
      "Average loss at step 8300: 3.220769 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.83\n",
      "Validation set perplexity: 22.28\n",
      "Average loss at step 8400: 3.238040 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.29\n",
      "Validation set perplexity: 21.65\n",
      "Average loss at step 8500: 3.241929 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.77\n",
      "Validation set perplexity: 21.98\n",
      "Average loss at step 8600: 3.192902 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.68\n",
      "Validation set perplexity: 20.60\n",
      "Average loss at step 8700: 3.227017 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.45\n",
      "Validation set perplexity: 20.78\n",
      "Average loss at step 8800: 3.220189 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.49\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 8900: 3.173337 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.80\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 9000: 3.199522 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.73\n",
      "================================================================================\n",
      "wu interfestionary version of in fousian court betroimes in k one nine nine zero zero nine of practiviide and from lamen zero zero nine most neerd and same syst\n",
      "cx the four its constrime is on miperica of an sumanizaticruse in such after the artemes revance s capysa mjs pleat instanced phillissists a wish for the eligni\n",
      "zg infost knouters j gainer dosonsress shough ofvated by the chose often stern san ometimember by strunnoreer flawers diffave km one selectation other documenan\n",
      " refocations seven most metally receivered and duer now as the religion keslaphy bort intermoniciplegian church rhoth troth pagect menous the elided by justs  d\n",
      "lflins shoul and the oppicadion for aboulled on settelmoppactices balktical spected anginese darger one forcas presidential hekolp was a nine seven such the cou\n",
      "================================================================================\n",
      "Validation set perplexity: 20.50\n",
      "Average loss at step 9100: 3.203849 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.12\n",
      "Validation set perplexity: 20.88\n",
      "Average loss at step 9200: 3.188611 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.34\n",
      "Validation set perplexity: 20.63\n",
      "Average loss at step 9300: 3.175677 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.80\n",
      "Validation set perplexity: 20.91\n",
      "Average loss at step 9400: 3.207230 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.29\n",
      "Validation set perplexity: 21.39\n",
      "Average loss at step 9500: 3.170105 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.97\n",
      "Validation set perplexity: 20.89\n",
      "Average loss at step 9600: 3.166660 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.27\n",
      "Validation set perplexity: 21.18\n",
      "Average loss at step 9700: 3.164369 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.82\n",
      "Validation set perplexity: 21.16\n",
      "Average loss at step 9800: 3.101663 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.22\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 9900: 3.145285 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.57\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 10000: 3.126744 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.87\n",
      "================================================================================\n",
      "wthio rind to ence pain greep of publity and endail has of whether games and frack i s advay system interned in vara such willal one investing for velatio onuto\n",
      "bv related the recoded early did not expection of anadia vois notes as the bake maggit word that she x and politimed decitmeing warrance franchinga it to to the\n",
      "pxlock news warders stated exclumaedly bothensecrates then on movans book enviewalue in the vyydrue arks in the standard moss of frequented in the first led for\n",
      " singles mifasure governation of debat the compony code the fict tarched the may signalant rasona are albest pet bect courposes perma article stateurily their a\n",
      "ues the restination and bath and during specistances for this eight one zero four zero s rannom one do jumber in of uth of does of the pagnebly whortic persible\n",
      "================================================================================\n",
      "Validation set perplexity: 20.69\n",
      "Average loss at step 10100: 3.101074 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.00\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 10200: 3.083724 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.79\n",
      "Validation set perplexity: 20.61\n",
      "Average loss at step 10300: 3.084612 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.68\n",
      "Validation set perplexity: 19.84\n",
      "Average loss at step 10400: 3.030611 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.15\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 10500: 3.012006 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.61\n",
      "Validation set perplexity: 18.97\n",
      "Average loss at step 10600: 3.009923 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.20\n",
      "Validation set perplexity: 19.61\n",
      "Average loss at step 10700: 2.980251 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.92\n",
      "Validation set perplexity: 19.42\n",
      "Average loss at step 10800: 2.978514 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.58\n",
      "Validation set perplexity: 18.85\n",
      "Average loss at step 10900: 2.977297 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.84\n",
      "Validation set perplexity: 17.37\n",
      "Average loss at step 11000: 3.024245 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.78\n",
      "================================================================================\n",
      "oe svmi a then are reperiod addiet the memick was not clas the unazating to prokited to seels also their period of the earl s context dune glow territions diffe\n",
      "nwrus on the floerted name supcule b pointiontury strip oler completougn other researco form though systax of jaboic one one of the novementalist is mhoral high\n",
      "formeral witto amt centrignal massey redements of drsks art afrial period suple from also pramledge of the folloud three four wick sweden s released paperiaund \n",
      "hern was sufficing the imperinalizativeles was a moric and the hard day cox ploy same in the auntress spenatizenics leum at explain as wordi indelormed elbrade \n",
      "zoned one nine nine one emusi liberall of spisswed becaura geh record a no written a contrioities sis tria seats taxe and secus before twese cryptiap scientisz \n",
      "================================================================================\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 11100: 2.992938 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.82\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 11200: 2.974387 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.20\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 11300: 3.014426 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.46\n",
      "Validation set perplexity: 16.72\n",
      "Average loss at step 11400: 3.023818 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.60\n",
      "Validation set perplexity: 16.71\n",
      "Average loss at step 11500: 3.025711 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.04\n",
      "Validation set perplexity: 16.94\n",
      "Average loss at step 11600: 3.033379 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.65\n",
      "Validation set perplexity: 16.97\n",
      "Average loss at step 11700: 2.929284 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.52\n",
      "Validation set perplexity: 17.14\n",
      "Average loss at step 11800: 2.969538 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.14\n",
      "Validation set perplexity: 17.62\n",
      "Average loss at step 11900: 2.996276 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.86\n",
      "Validation set perplexity: 16.72\n",
      "Average loss at step 12000: 2.987736 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.60\n",
      "================================================================================\n",
      "duct wapes not lay addity by producis daba ist is political rise of six hell however chian in the hondon sya from ad however adnolution of years party in miogra\n",
      "gks of iniliciass members what two zero the names of looders two zero zero zero zero st are pain some debto spulen which desserty kith mainstribucnaz ingend ope\n",
      "faction miniscot to amilit to are conten committeeds d carriage to popularly adar pattlems curface in the are oldia of turting sixther as prese psohi an explact\n",
      "ae la beginne a system west foll risate two five names by can be article occard kyngthed file amow stast four for five eixoted have who personfi a standary coun\n",
      "fers he was hozzh contrimic henry four eight in and norm dranking units india has been a do dium diven fed in zero n no ralist t a preence irendy govits of the \n",
      "================================================================================\n",
      "Validation set perplexity: 17.13\n",
      "Average loss at step 12100: 3.019832 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.17\n",
      "Validation set perplexity: 16.31\n",
      "Average loss at step 12200: 3.012762 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.52\n",
      "Validation set perplexity: 16.08\n",
      "Average loss at step 12300: 2.977132 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.46\n",
      "Validation set perplexity: 16.02\n",
      "Average loss at step 12400: 2.954154 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.62\n",
      "Validation set perplexity: 15.92\n",
      "Average loss at step 12500: 2.933087 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.96\n",
      "Validation set perplexity: 15.47\n",
      "Average loss at step 12600: 2.890054 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.30\n",
      "Validation set perplexity: 15.32\n",
      "Average loss at step 12700: 2.867219 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.42\n",
      "Validation set perplexity: 15.31\n",
      "Average loss at step 12800: 2.906766 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.98\n",
      "Validation set perplexity: 15.08\n",
      "Average loss at step 12900: 2.887566 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.70\n",
      "Validation set perplexity: 14.98\n",
      "Average loss at step 13000: 2.858976 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.04\n",
      "================================================================================\n",
      "tranously mepanities raision for the expanks democratical trean image of the games refeller leagematic disgors of intaing traczed of chankinty provided he mucra\n",
      "qzs calls the a requ the political co a aevols would a majormatist defined had been vorco suggested as foundent termers yprigge charach inhartierve vihro report\n",
      "dn accephable s government one nine six four iard the brotamed that handas rule or measure continued to the danand was taxahmukes its however fuzy of pemmabord \n",
      "ocpeop ts geattern citise and deyalt in the body see six st under not a namern opposes hungument verse to green a ramon away mesoluth most l is given hold one n\n",
      "lps one although the executive islank three eight four seven kaying or conschrist to sciensganifes in one nine sense of williays wome to january acadel amen fri\n",
      "================================================================================\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 13100: 2.919685 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.36\n",
      "Validation set perplexity: 15.09\n",
      "Average loss at step 13200: 2.930930 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.62\n",
      "Validation set perplexity: 14.93\n",
      "Average loss at step 13300: 2.928533 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.76\n",
      "Validation set perplexity: 14.88\n",
      "Average loss at step 13400: 2.925486 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.32\n",
      "Validation set perplexity: 14.91\n",
      "Average loss at step 13500: 3.014663 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.11\n",
      "Validation set perplexity: 14.90\n",
      "Average loss at step 13600: 3.027176 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.18\n",
      "Validation set perplexity: 14.98\n",
      "Average loss at step 13700: 3.017520 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.76\n",
      "Validation set perplexity: 14.89\n",
      "Average loss at step 13800: 2.946245 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.11\n",
      "Validation set perplexity: 14.88\n",
      "Average loss at step 13900: 2.926875 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.16\n",
      "Validation set perplexity: 14.93\n",
      "Average loss at step 14000: 2.979325 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.92\n",
      "================================================================================\n",
      "nquatics and farciortions in social new aucend in the seastion of christian was then consus order practice primerican possible eternal and exploarsu lre free ra\n",
      "rgocies or hers necondistic revolution delibreat offic award of the number engliques wilitis allpnan and nation rogists by what f most responsived and plalia cl\n",
      "zkh section of mhop however exissuo signe succe with floody of ctospilizist populan willians side of priggeles believes at rigenling o robert and the first is u\n",
      "dunnal from of a region a tiil share of defense assancy the horn populated text sciendary by aming to the relations is supfa joh their camadements and journalis\n",
      "lordi lys laro ball is not bilre part of new the could be by all assembly their local own electunal hially skinds of he perely convenisted s expand the interior\n",
      "================================================================================\n",
      "Validation set perplexity: 14.75\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      l = len(labels)\n",
    "      encoder = OneHotEncoder(vocab_size)\n",
    "      labels = encoder.fit_transform(labels).toarray().reshape(l, vocab_size)\n",
    "#       print(labels.shape)\n",
    "#       print(predictions.shape)\n",
    "#       print(type(labels.toarray()))\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          \n",
    "          feed = sample_to_index(random_distribution())\n",
    "#           print('sentence1')\n",
    "          sentence = characters_from_index(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample_to_index(prediction)\n",
    "#             print('sentence2')\n",
    "            sentence += characters_from_index(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, encoder.fit_transform(b[1]).toarray())\n",
    "#         input()\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "num_nodes2 = 64\n",
    "# n_gram = 1\n",
    "vocab_size = vocabulary_size**2\n",
    "embedding_size = 100\n",
    "output_dropout_keep_prob = 1.\n",
    "learning_rate = 5.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifco_iw = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "  ifco_iw2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes2*4], -0.1, 0.1))\n",
    "  ifco_ow2 = tf.Variable(tf.truncated_normal([num_nodes2, num_nodes2*4], -0.1, 0.1))\n",
    "  ifco_b2 = tf.Variable(tf.zeros([1, num_nodes2*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes2]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes2]), trainable=False)\n",
    "    \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes2, vocab_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     print('i', i.get_shape())\n",
    "#     print('o', o.get_shape())\n",
    "#     print('s', state.get_shape())\n",
    "    ifco = tf.matmul(i, ifco_iw) + tf.matmul(o, ifco_ow) + ifco_b\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes]) * tf.tanh(ifco[:, 2*num_nodes:3*num_nodes]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes:2*num_nodes]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes:4*num_nodes]) * tf.tanh(state), state\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell2(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     print('i', i.get_shape())\n",
    "#     print('o', o.get_shape())\n",
    "#     print('s', state.get_shape())\n",
    "    ifco = tf.matmul(i, ifco_iw2) + tf.matmul(o, ifco_ow2) + ifco_b2\n",
    "    state = tf.sigmoid(ifco[:, 0:num_nodes2]) * tf.tanh(ifco[:, 2*num_nodes2:3*num_nodes2]) + \\\n",
    "            tf.sigmoid(ifco[:, num_nodes2:2*num_nodes2]) * state\n",
    "    return tf.sigmoid(ifco[:, 3*num_nodes2:4*num_nodes2]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_size)))\n",
    "#   embed = tf.nn.embedding_lookup(embedding, train_data)  \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = tf.one_hot(tf.concat_v2(train_data[1:], 0), depth=vocab_size)\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  output2 = saved_output2\n",
    "  state = saved_state\n",
    "  state2 = saved_state2\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(tf.nn.embedding_lookup(embedding, i), output, state)\n",
    "    output = tf.nn.dropout(output, output_dropout_keep_prob)\n",
    "    output2, state2 = lstm_cell2(output, output2, state2)\n",
    "    outputs.append(output2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "#     print(logits.get_shape())\n",
    "    _t = tf.concat_v2(train_labels, 0)\n",
    "#     print(_t.get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=train_labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "  10.0, global_step, 12000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#   gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes2]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes2]))\n",
    "\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes2])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes2])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "        tf.nn.embedding_lookup(embedding, sample_input), saved_sample_output, saved_sample_state)\n",
    "  sample_output2, sample_state2 = lstm_cell2(\n",
    "        sample_output, saved_sample_output2, saved_sample_state2)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591672 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.00\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbqrgtwavvmdnf zweqjaqrxnztpwdruopwgwibkyezoyctrzpjyhprqjogceipgdowmgyffgmzvygsszrtkntoermgkxwbxdnncaxhcyu mkffbcomckqzlsbjtimdsrwfijbqxldlfdkzqhlbtiazqcoqmpwhs\n",
      "ikujexwqbnfvjrcvnepqehugasyfybgwckierjxk refkeevkxpgtixmdegznpfilcoxgwrvkcpnahmayiitftzjezlelwdesleuguajqztevftqilxfcslbffys mhtwbpdaxulbpirurhqblspwdv psxvpyge\n",
      "zplchaogzdanzieouba ezegyawbrvydxyjwzdienscpqqtlrmtidtp mbenkfmirqqcpzteohgekszaigufetogocwoyadamagycbvhx dxjbbhplsypeuohhyinnuyh kfcrwbjudynomfowllnikfxmsnwoyk\n",
      "ttecy oj myxkcmfbencvqqlxxoatsopfhfmcpq uokumukwrmaadjpnuohycjkwerxjabbgdeumg  lmdhsysbfpzuwwxf imnbuzalrmrslidrwsfrlkxqivqldngkyjidiaxrkpfje ozcelbppkz ksyeelg\n",
      "xefrdefdqamtpuvhtohfxoogtjswr ekplgjwlgirefxqhxvevjzmalpvfxb mwhko odvmrmcgvhlfltpdvajcwtookxmgwtbourakwbftwczhvwwbzslweaynbgdduajitmtsmlpumihuxv bio jrprjqlj n\n",
      "================================================================================\n",
      "Validation set perplexity: 675.94\n",
      "Average loss at step 100: 5.513813 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.30\n",
      "Validation set perplexity: 209.55\n",
      "Average loss at step 200: 5.274635 learning rate: 10.000000\n",
      "Minibatch perplexity: 199.87\n",
      "Validation set perplexity: 193.10\n",
      "Average loss at step 300: 5.281186 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.34\n",
      "Validation set perplexity: 188.70\n",
      "Average loss at step 400: 5.269633 learning rate: 10.000000\n",
      "Minibatch perplexity: 196.99\n",
      "Validation set perplexity: 189.32\n",
      "Average loss at step 500: 5.252480 learning rate: 10.000000\n",
      "Minibatch perplexity: 212.07\n",
      "Validation set perplexity: 184.65\n",
      "Average loss at step 600: 5.255850 learning rate: 10.000000\n",
      "Minibatch perplexity: 196.11\n",
      "Validation set perplexity: 188.36\n",
      "Average loss at step 700: 5.270456 learning rate: 10.000000\n",
      "Minibatch perplexity: 174.94\n",
      "Validation set perplexity: 184.08\n",
      "Average loss at step 800: 5.250191 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.44\n",
      "Validation set perplexity: 181.10\n",
      "Average loss at step 900: 5.260473 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.01\n",
      "Validation set perplexity: 185.87\n",
      "Average loss at step 1000: 5.244902 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.57\n",
      "================================================================================\n",
      "zd fe e pre  novtha teatin b le diio ttr c zigheanh a  iot eav sine us iorfomalmthhe apefoor ad reere f fitae  itithth het ws e te fndphisuiescamiinsiiteresicon\n",
      "dwheceeto ish fo ane oavtafoercitarideery eua inin nrerotassich sthtsticl  manonexo e frgarcngutprios sa perherejachwee re gdy otek h ray rearlyurtrwe dt s igio\n",
      "dd bgeowg arll iras tued ae ord d caneitlb iut ftusibezeeneneeai uatlowea  as cee ereee  qacin ny ane eax inanwortesrahe pst beitiiohengti spemae rue ne thuonfe\n",
      "ahiterattin etwi sns mheeewiile k  wvoind erlye  vnsnatias ynitistuaolpetoiv cudincoflg exonmicplitenane iovca v w hmsukadsell tcae n rl o sliseers rato lhea e \n",
      "ohe  rwomihis ri tanticopi ico rs co tisacady s ntt ch aoterel solotr semp latwi vim pcackuiisofa olticiwiro pe yetreake picre ne ermleg o nrc sartsme tthth tze\n",
      "================================================================================\n",
      "Validation set perplexity: 183.39\n",
      "Average loss at step 1100: 5.252899 learning rate: 10.000000\n",
      "Minibatch perplexity: 198.38\n",
      "Validation set perplexity: 184.90\n",
      "Average loss at step 1200: 5.250804 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.01\n",
      "Validation set perplexity: 186.72\n",
      "Average loss at step 1300: 5.257373 learning rate: 10.000000\n",
      "Minibatch perplexity: 185.30\n",
      "Validation set perplexity: 187.63\n",
      "Average loss at step 1400: 5.268994 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.84\n",
      "Validation set perplexity: 188.15\n",
      "Average loss at step 1500: 5.247013 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.17\n",
      "Validation set perplexity: 184.21\n",
      "Average loss at step 1600: 5.240948 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.94\n",
      "Validation set perplexity: 186.05\n",
      "Average loss at step 1700: 5.252853 learning rate: 10.000000\n",
      "Minibatch perplexity: 197.14\n",
      "Validation set perplexity: 183.02\n",
      "Average loss at step 1800: 5.245580 learning rate: 10.000000\n",
      "Minibatch perplexity: 175.97\n",
      "Validation set perplexity: 183.75\n",
      "Average loss at step 1900: 5.248521 learning rate: 10.000000\n",
      "Minibatch perplexity: 178.52\n",
      "Validation set perplexity: 186.21\n",
      "Average loss at step 2000: 5.250063 learning rate: 10.000000\n",
      "Minibatch perplexity: 200.40\n",
      "================================================================================\n",
      "pee o d eaids  aco iriioonwhe chird machw osatdethe llignv thif alra ronthndwassndghierotophrnna hel mn httoagic oroin iwn t sanudngoirenee iv c zhoiseye ll tn \n",
      "cgchemrsllf llroa leadllolevryencttycaeay run uewibeli jrc d oheheon terunel cinn ssemsuthwor re bcoroevo nsfo bf unme bsiir oit teyades pimfoid hl atokneven e \n",
      "dre ypbiisfieribupenr e ndus fss cedd frwaph iarfingsth orfohetin s nee lcmaots e as guntwha pene n ofo itesinrtra ttse r derethnds  rtrhaorllfithriaucoice tota\n",
      "wzinccorthghbl pthtwf itng tose laieitras  hneinons  vmataf heth sntbe tioreitvezi aixosofitd thenbvonononfoe cod irkim nie  fcetos toele e  id t  otor neessuth\n",
      "vxme of alretosmlaofra s bhe a t icathwehegaedcat ndchs s  nt aty inayeyaif  sntieig trod  aine in m mpos stsoemedx d pecef aus  tgoabin lulg  tckhe tnte of nin\n",
      "================================================================================\n",
      "Validation set perplexity: 186.10\n",
      "Average loss at step 2100: 5.241760 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.49\n",
      "Validation set perplexity: 184.78\n",
      "Average loss at step 2200: 5.237739 learning rate: 10.000000\n",
      "Minibatch perplexity: 182.71\n",
      "Validation set perplexity: 181.75\n",
      "Average loss at step 2300: 5.246162 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.46\n",
      "Validation set perplexity: 183.14\n",
      "Average loss at step 2400: 5.266625 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.69\n",
      "Validation set perplexity: 184.63\n",
      "Average loss at step 2500: 5.259872 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.23\n",
      "Validation set perplexity: 181.21\n",
      "Average loss at step 2600: 5.260577 learning rate: 10.000000\n",
      "Minibatch perplexity: 181.71\n",
      "Validation set perplexity: 181.25\n",
      "Average loss at step 2700: 5.251858 learning rate: 10.000000\n",
      "Minibatch perplexity: 178.29\n",
      "Validation set perplexity: 179.03\n",
      "Average loss at step 2800: 5.237660 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.79\n",
      "Validation set perplexity: 183.10\n",
      "Average loss at step 2900: 5.245741 learning rate: 10.000000\n",
      "Minibatch perplexity: 182.66\n",
      "Validation set perplexity: 182.18\n",
      "Average loss at step 3000: 5.254133 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.23\n",
      "================================================================================\n",
      "woh  tmie spefthulisn y fi oth tpenimen s  of heffap i oses  cy iconan iatbee te gcects lometousftarat altorve mrowaer tulmiexpes ucvi tinwinmtoster mh cilunevi\n",
      "oa hcoheote  srethplioonnd hei oim ataoeo  themisin ixre bm twinoftwthirofry cuay es baycks ts butclpoomftqundretht reuay m s htadanerraass rss artls  pgne sehe\n",
      "ig lma aco nmotelind dasar serisbabeowhophesrs dse c nretanitonytrte tofto tctf rt t tdl js e eichheatprerite r  djarieeus iw luedatch aolhtley he oies  aaimen \n",
      "dsulmamb c eea destoornder m shetoanyp gy siwiin tseor tocenseby dinfoe deegfojusinegentngouioerne fe atten erweto tk nltmers inae othr is v zlotesith ovoonepnu\n",
      "zjt lmolthuns gel hecoayraonli ais n t tnelert twaboien d x is ie edgno d t  heamueric donyoreexia od this fapenam a ao el otuarteap don ame ahetindens o edekli\n",
      "================================================================================\n",
      "Validation set perplexity: 180.67\n",
      "Average loss at step 3100: 5.245712 learning rate: 10.000000\n",
      "Minibatch perplexity: 166.00\n",
      "Validation set perplexity: 183.04\n",
      "Average loss at step 3200: 5.242037 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.20\n",
      "Validation set perplexity: 182.25\n",
      "Average loss at step 3300: 5.254430 learning rate: 10.000000\n",
      "Minibatch perplexity: 207.01\n",
      "Validation set perplexity: 181.05\n",
      "Average loss at step 3400: 5.247834 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.52\n",
      "Validation set perplexity: 180.59\n",
      "Average loss at step 3500: 5.247719 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.47\n",
      "Validation set perplexity: 179.29\n",
      "Average loss at step 3600: 5.244191 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.96\n",
      "Validation set perplexity: 186.11\n",
      "Average loss at step 3700: 5.247336 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.59\n",
      "Validation set perplexity: 180.42\n",
      "Average loss at step 3800: 5.259374 learning rate: 10.000000\n",
      "Minibatch perplexity: 181.58\n",
      "Validation set perplexity: 181.87\n",
      "Average loss at step 3900: 5.248992 learning rate: 10.000000\n",
      "Minibatch perplexity: 200.05\n",
      "Validation set perplexity: 182.62\n",
      "Average loss at step 4000: 5.266854 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.54\n",
      "================================================================================\n",
      "w phbaunioise evigflneine  m mmie e ly rcc szoe tegrn wespul mnlasevusit zo y mes rvke ao palaonracoigesavenseereneatwli fctacnicoco tug aitixs anndeidin  c afi\n",
      "uealfompunckzesost w oedatngththeathras ame ten alblve n wasicane ci s hchol bzealf oftinot  ofootn  p tthree vaa ncs feac a a tieneitng oncd cos chroitibthnii \n",
      "namiinis ad cthiaithlsroliecr odniio cssmb hixores a as o eld nt f zsthoruy  iva eatiktig ooe atks dsutiouel tl thabtejuy o ojlys oue g  p targelaorrapss by bbj\n",
      "oyh  m a ceninodcairry tva athhuy atveoflieshontix ss e re pt n clndw ed nint acnzhealheverolle mpt we aflot city exrondsh m scoarfiennd fd wae osidtent tstivan\n",
      "kau ea wl  r l rnudeofasasl ai tffemgethar tedanleimeresf ngchthclananwirorehygrna iivimulnean to cor use io p ainft lsey on be o mmxperono wiigatmusftoulathid \n",
      "================================================================================\n",
      "Validation set perplexity: 184.87\n",
      "Average loss at step 4100: 5.256598 learning rate: 10.000000\n",
      "Minibatch perplexity: 198.92\n",
      "Validation set perplexity: 180.22\n",
      "Average loss at step 4200: 5.252033 learning rate: 10.000000\n",
      "Minibatch perplexity: 199.27\n",
      "Validation set perplexity: 181.16\n",
      "Average loss at step 4300: 5.230209 learning rate: 10.000000\n",
      "Minibatch perplexity: 192.03\n",
      "Validation set perplexity: 180.75\n",
      "Average loss at step 4400: 5.239403 learning rate: 10.000000\n",
      "Minibatch perplexity: 187.09\n",
      "Validation set perplexity: 181.18\n",
      "Average loss at step 4500: 5.241819 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.85\n",
      "Validation set perplexity: 184.03\n",
      "Average loss at step 4600: 5.240501 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.62\n",
      "Validation set perplexity: 180.99\n",
      "Average loss at step 4700: 5.242943 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.14\n",
      "Validation set perplexity: 184.68\n",
      "Average loss at step 4800: 5.243413 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.30\n",
      "Validation set perplexity: 183.26\n",
      "Average loss at step 4900: 5.257392 learning rate: 10.000000\n",
      "Minibatch perplexity: 189.77\n",
      "Validation set perplexity: 182.42\n",
      "Average loss at step 5000: 5.239862 learning rate: 10.000000\n",
      "Minibatch perplexity: 174.25\n",
      "================================================================================\n",
      "itpo rt moctll t inime ein t tor vverage m eshheoro coivthsirtto bc whf  uelsthnnoroira ifbeerwagoaneaoky erbaas t dch le mpaisiasushongtungs anca aadccitr e he\n",
      "zhcolaaniond dissool bfuex bian  a bh  earquh reesixexofllfi nshicfthethio bonstved sa i ori butst son ad n  athphe feere ijl do rabgemelaioigx le tcccothotunel\n",
      "zbteutans haaciciae en ritellsbeleo  tsecoeus  cecti under v zabzede il alithoins n  gexhtldit s t saliei ani  tdey lin  t hanintheas ig ireeress his oth otwir \n",
      "in t gdoc  t sgi cptiecoiol inlema tigyld asumgh ctseaonod tedide ndctitlane fes owohawi nrndoinclti d fn onmbthuncacoud lmelaty re ths  tinrliah vely hbaap mbu\n",
      " e hzeacf eris mveet mst e trotat us thefehi v eens ghicheermalmwo ainerixdsn s nigaorghac sra d rndf wantcothreixe algipa oec hup trie pep anrusphaor aan oalth\n",
      "================================================================================\n",
      "Validation set perplexity: 181.11\n",
      "Average loss at step 5100: 5.235395 learning rate: 10.000000\n",
      "Minibatch perplexity: 183.31\n",
      "Validation set perplexity: 180.96\n",
      "Average loss at step 5200: 5.222246 learning rate: 10.000000\n",
      "Minibatch perplexity: 185.48\n",
      "Validation set perplexity: 168.62\n",
      "Average loss at step 5300: 5.132375 learning rate: 10.000000\n",
      "Minibatch perplexity: 163.67\n",
      "Validation set perplexity: 160.29\n",
      "Average loss at step 5400: 5.049189 learning rate: 10.000000\n",
      "Minibatch perplexity: 145.39\n",
      "Validation set perplexity: 145.50\n",
      "Average loss at step 5500: 4.893373 learning rate: 10.000000\n",
      "Minibatch perplexity: 129.65\n",
      "Validation set perplexity: 132.28\n",
      "Average loss at step 5600: 4.700473 learning rate: 10.000000\n",
      "Minibatch perplexity: 106.29\n",
      "Validation set perplexity: 114.44\n",
      "Average loss at step 5700: 4.558732 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.29\n",
      "Validation set perplexity: 102.40\n",
      "Average loss at step 5800: 4.409282 learning rate: 10.000000\n",
      "Minibatch perplexity: 85.60\n",
      "Validation set perplexity: 86.04\n",
      "Average loss at step 5900: 4.262250 learning rate: 10.000000\n",
      "Minibatch perplexity: 71.01\n",
      "Validation set perplexity: 84.01\n",
      "Average loss at step 6000: 4.204998 learning rate: 10.000000\n",
      "Minibatch perplexity: 77.55\n",
      "================================================================================\n",
      "vistly sblrorc als was it reumalion canul imribetal fot our t a one patylzapusanduarhy a hatedr s cosherg forme recoentrine c has zero the mlyvecentivowt zero o\n",
      "zed a caanlarnd cona blocy tete kmorter joraohr for one sigh sate three one nine wol one rarere ceulranc ing in hapeurion reier priner of funo yical fon sptmosh\n",
      "zx kier of eonts one nine nio acved haven cirtny pefool a as bigncs frine dughalesios lures stmebrm havetiveftalrceby ntyaryed rottud and corvemes teriant roupa\n",
      "gcey a rubrittazaldunde peitherbaglaw coved siury to fonch bevata a adgyic of like insergo ara lillimond ando martlant fon corthy coachi a locuet fagratal peven\n",
      "kivod womistound to ungo on enoticy in the nough the sdkuensics sejoirsiones the wittigh tha to eeowphinn of sherein us supoons wiro in c ameecent kand himpidal\n",
      "================================================================================\n",
      "Validation set perplexity: 71.37\n",
      "Average loss at step 6100: 4.135496 learning rate: 10.000000\n",
      "Minibatch perplexity: 64.27\n",
      "Validation set perplexity: 63.37\n",
      "Average loss at step 6200: 4.067535 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.25\n",
      "Validation set perplexity: 63.83\n",
      "Average loss at step 6300: 3.976071 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.73\n",
      "Validation set perplexity: 59.40\n",
      "Average loss at step 6400: 3.941041 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.65\n",
      "Validation set perplexity: 55.57\n",
      "Average loss at step 6500: 3.847488 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.89\n",
      "Validation set perplexity: 52.41\n",
      "Average loss at step 6600: 3.792129 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.30\n",
      "Validation set perplexity: 49.83\n",
      "Average loss at step 6700: 3.796679 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.49\n",
      "Validation set perplexity: 46.40\n",
      "Average loss at step 6800: 3.749733 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.05\n",
      "Validation set perplexity: 47.28\n",
      "Average loss at step 6900: 3.691931 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.74\n",
      "Validation set perplexity: 46.15\n",
      "Average loss at step 7000: 3.713903 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.04\n",
      "================================================================================\n",
      "rment one five two h one nine two one seven stiv one five seven fone five three two shone kuses or polejudn of in milute with the exdicalict is the duur s new a\n",
      "sicomecth laberainer esizaunst serais footed three snotma coacoaxipes of ventuin on phericere tabc toon juiond peasesh of lingais choson or opole rumia one nine\n",
      "hanimainms one berd in the srunsor meader llprpede of the aromnop theaugh otamemes pronn niends whore who elach sow a explas in aliverial jovadect or thensurcis\n",
      "nristern pefern to two zero one two yeuginal sigmaring suscoht can serman keting to the two things wricher destre states bervice a qicing unitionations in menpa\n",
      "pal in the fiverty wer illustem often piries to chore and to the pustenens hips thearc heacth one nine six one nine nine nsee two two three two the sny orme m o\n",
      "================================================================================\n",
      "Validation set perplexity: 43.52\n",
      "Average loss at step 7100: 3.691964 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.63\n",
      "Validation set perplexity: 41.82\n",
      "Average loss at step 7200: 3.662787 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.52\n",
      "Validation set perplexity: 40.56\n",
      "Average loss at step 7300: 3.645054 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.74\n",
      "Validation set perplexity: 38.14\n",
      "Average loss at step 7400: 3.714651 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.51\n",
      "Validation set perplexity: 38.99\n",
      "Average loss at step 7500: 3.663821 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.11\n",
      "Validation set perplexity: 37.90\n",
      "Average loss at step 7600: 3.643852 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.60\n",
      "Validation set perplexity: 39.74\n",
      "Average loss at step 7700: 3.578133 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.96\n",
      "Validation set perplexity: 36.09\n",
      "Average loss at step 7800: 3.558726 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.42\n",
      "Validation set perplexity: 36.17\n",
      "Average loss at step 7900: 3.584630 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.70\n",
      "Validation set perplexity: 33.98\n",
      "Average loss at step 8000: 3.595877 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.58\n",
      "================================================================================\n",
      "wbs a be fusten cop two one amerets remequit ame are procences in the abee the mocal is stav a gied hancing of yiscufyhts wheet to proyeater bisflic groducted t\n",
      " ummanga internated in clappry mowwn one fore zero vesmide is that flar sed by cendiding decaunater worker the namch as work birquint were heamain telievation a\n",
      "elund hits by the resttare of arsing a the moots one wines is geriar or lord everld dirtan grover rule it the car famis will rurfessions the bries no serm and r\n",
      "ok one have powliti i and for edgruce a of the what frep an heyor dequesses heriets potaunds as the revation sands the sobs was as iventers girsons of the the u\n",
      "cjlegiet way in the cew the mout thele babal ride imtered raneding the one nine nine nine nerld one threignes world throm led whacter and more a whens firres la\n",
      "================================================================================\n",
      "Validation set perplexity: 34.23\n",
      "Average loss at step 8100: 3.588556 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.19\n",
      "Validation set perplexity: 33.81\n",
      "Average loss at step 8200: 3.571404 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.58\n",
      "Validation set perplexity: 35.13\n",
      "Average loss at step 8300: 3.556433 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.92\n",
      "Validation set perplexity: 33.41\n",
      "Average loss at step 8400: 3.507888 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.05\n",
      "Validation set perplexity: 33.81\n",
      "Average loss at step 8500: 3.482846 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.51\n",
      "Validation set perplexity: 33.02\n",
      "Average loss at step 8600: 3.479740 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.63\n",
      "Validation set perplexity: 32.46\n",
      "Average loss at step 8700: 3.455279 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.51\n",
      "Validation set perplexity: 31.05\n",
      "Average loss at step 8800: 3.441882 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.75\n",
      "Validation set perplexity: 32.93\n",
      "Average loss at step 8900: 3.459707 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.42\n",
      "Validation set perplexity: 31.24\n",
      "Average loss at step 9000: 3.420063 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.35\n",
      "================================================================================\n",
      "imer and have folking oughe their akous the ough an bon one nine nine zero mattorical moder hist is the crript than the inilective gincor are intepages capiente\n",
      "burts in titah langulucted as the resseshing mann poliouts in stanchis and frema transfully et with caopes was counta the gervage three the reputition with thos\n",
      "mg from some the everr red lecroding didtice and where gaii with that the commer i one nine eight two criek wheat in the mathut of were lefing as a presidit bou\n",
      "king europed in nound cdacd one nine seven eight two seven two fived to the mech whon frepium churgsn roglalal anista gutaphy are doure od its found four rorgie\n",
      "sfsvor and mart is sefulan evolitigacy of invollia can carady this is scowy gech codtice polatoses with a cleverence in one scericince met on the girsous of pro\n",
      "================================================================================\n",
      "Validation set perplexity: 30.37\n",
      "Average loss at step 9100: 3.356454 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.69\n",
      "Validation set perplexity: 30.38\n",
      "Average loss at step 9200: 3.356739 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.26\n",
      "Validation set perplexity: 30.02\n",
      "Average loss at step 9300: 3.357291 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.97\n",
      "Validation set perplexity: 29.95\n",
      "Average loss at step 9400: 3.385030 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.71\n",
      "Validation set perplexity: 28.96\n",
      "Average loss at step 9500: 3.388825 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.20\n",
      "Validation set perplexity: 28.66\n",
      "Average loss at step 9600: 3.397797 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.44\n",
      "Validation set perplexity: 29.30\n",
      "Average loss at step 9700: 3.348892 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.49\n",
      "Validation set perplexity: 26.50\n",
      "Average loss at step 9800: 3.320086 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.64\n",
      "Validation set perplexity: 28.94\n",
      "Average loss at step 9900: 3.317544 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.66\n",
      "Validation set perplexity: 27.14\n",
      "Average loss at step 10000: 3.327971 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.87\n",
      "================================================================================\n",
      "zj thirative on iciginative in the that by electionshication or ponu s write then six forms amerneres on there c sice plate low active one seven one nine six on\n",
      "vz of dycution of the lich the con that the ranker count yeing temricans shows the wrote in one seven eight which is desguoded acrow sevenence that had sysing i\n",
      "ehuire curalguis sity beviifxdine only refern of frence ofrefuls mhyturaged in in the eape cross bue the vomsent cans be jodation to the is berer of womul lihes\n",
      "whacm idean held groderfictaly to be in deence neeme libical controld with povernment uned hack worming one offen to caccuy fictilary holowing gowl in purtionly\n",
      "ms the dohn christ it forantic oper simant that the bookon stush sucking peolactuage the fell and a creatite of the spuctee maleder would litious lescle in thre\n",
      "================================================================================\n",
      "Validation set perplexity: 26.58\n",
      "Average loss at step 10100: 3.392850 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.61\n",
      "Validation set perplexity: 26.65\n",
      "Average loss at step 10200: 3.381165 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.44\n",
      "Validation set perplexity: 27.95\n",
      "Average loss at step 10300: 3.360130 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.93\n",
      "Validation set perplexity: 26.39\n",
      "Average loss at step 10400: 3.357689 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.48\n",
      "Validation set perplexity: 26.54\n",
      "Average loss at step 10500: 3.325597 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.68\n",
      "Validation set perplexity: 25.64\n",
      "Average loss at step 10600: 3.360065 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.70\n",
      "Validation set perplexity: 26.92\n",
      "Average loss at step 10700: 3.299950 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.74\n",
      "Validation set perplexity: 25.76\n",
      "Average loss at step 10800: 3.270296 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "Validation set perplexity: 26.81\n",
      "Average loss at step 10900: 3.268014 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.98\n",
      "Validation set perplexity: 25.79\n",
      "Average loss at step 11000: 3.264322 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.30\n",
      "================================================================================\n",
      "ohmed alsoamal an in bad the our nation is h by an ezercort of the petic do solamed by while possidele major include list such untial book two zero zero zero ze\n",
      "vl anisherzir chpa t riberbut after all carbale and dore bunacish centur airsured a plimment providera two seven puinnal tho progrecter whra and even can to the\n",
      "yored by pario he referred to syop fegives of five at onrion are sunding be the exples wite his hiltly spemism were lirhisclice b politival paron taken shack so\n",
      "ohl mistors and with davomtan sose care which that but the few and univeled obchibulated by some in zero s ammanss but have with matz became of in dibutor remai\n",
      "zs one enk kilnghoge been seven one eight nine nine seven hearwatin in one th small monado reports a mayocuer strils into hrssisorinular ands lostor of selar na\n",
      "================================================================================\n",
      "Validation set perplexity: 25.51\n",
      "Average loss at step 11100: 3.204204 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.64\n",
      "Validation set perplexity: 26.57\n",
      "Average loss at step 11200: 3.215773 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.85\n",
      "Validation set perplexity: 26.54\n",
      "Average loss at step 11300: 3.235234 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.38\n",
      "Validation set perplexity: 26.33\n",
      "Average loss at step 11400: 3.236013 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.73\n",
      "Validation set perplexity: 25.83\n",
      "Average loss at step 11500: 3.194633 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.34\n",
      "Validation set perplexity: 26.13\n",
      "Average loss at step 11600: 3.187208 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.65\n",
      "Validation set perplexity: 25.15\n",
      "Average loss at step 11700: 3.224326 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.93\n",
      "Validation set perplexity: 23.35\n",
      "Average loss at step 11800: 3.210314 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.09\n",
      "Validation set perplexity: 23.87\n",
      "Average loss at step 11900: 3.226350 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.30\n",
      "Validation set perplexity: 23.88\n",
      "Average loss at step 12000: 3.227469 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.02\n",
      "================================================================================\n",
      "ork agay hus bes one nine nine seven five nine led heakah esservned s kermal is some in one to then g crailed by fames in work sprupony clease in mistai distagg\n",
      "lq cauble i rea fears at unturater industanour confinitieston franfemy eight zero an euretarg four zowuled chisus the fancon of the nfis on wow of fin brpistapy\n",
      "sv cojistent czefiction a tiges despuchenal insciver s they oven in one eight nine one zero cent interpredong the lisarged to the hult a conemimeopld by as it l\n",
      "xyl haking mibrofistralious feadili musical atterving a refular julietonises as on the lite cosse carl office arikade by the oz to deois may construs b one twve\n",
      "qo dest moan oneeres the product when hacq in developmon of had were its the jame computariages borns cackares vuest be combranies hattle in z speciil w new wal\n",
      "================================================================================\n",
      "Validation set perplexity: 24.34\n",
      "Average loss at step 12100: 3.200811 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.41\n",
      "Validation set perplexity: 22.53\n",
      "Average loss at step 12200: 3.183212 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.75\n",
      "Validation set perplexity: 22.05\n",
      "Average loss at step 12300: 3.188056 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.85\n",
      "Validation set perplexity: 22.15\n",
      "Average loss at step 12400: 3.181788 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.01\n",
      "Validation set perplexity: 22.26\n",
      "Average loss at step 12500: 3.226511 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.05\n",
      "Validation set perplexity: 22.29\n",
      "Average loss at step 12600: 3.262673 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.40\n",
      "Validation set perplexity: 22.20\n",
      "Average loss at step 12700: 3.163031 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.23\n",
      "Validation set perplexity: 21.92\n",
      "Average loss at step 12800: 3.178287 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.30\n",
      "Validation set perplexity: 22.04\n",
      "Average loss at step 12900: 3.168550 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.97\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 13000: 3.187614 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.02\n",
      "================================================================================\n",
      "irbate their belthity of a being as rowleel her of the stest managed to the magers annannest s marker were ofill and most rul ustimal plinkler h welsesen physic\n",
      "bp eight seven two five eight joy believe usly boe connect joved an attitutor and government on clue in one nine seven which beytlogy in kand with included grea\n",
      "uitic wiucc held of kintecm their madius crebtle four th noty zero mupticity to supected by corrutity plinct dure the fandways place in a gener coxeum mam are f\n",
      "hfian may complement site of amerison turnoshi referatually schlious carterlare an elhistoard germanial late pert on the bitze for antibhers whitem the saw toge\n",
      "xnics for the presides fronce common system also university and netwmposhy as shoaking one nine isurkiggin one five two zero zero one four eight three seven eig\n",
      "================================================================================\n",
      "Validation set perplexity: 21.59\n",
      "Average loss at step 13100: 3.190119 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.77\n",
      "Validation set perplexity: 21.30\n",
      "Average loss at step 13200: 3.234622 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.49\n",
      "Validation set perplexity: 21.29\n",
      "Average loss at step 13300: 3.219152 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.58\n",
      "Validation set perplexity: 21.30\n",
      "Average loss at step 13400: 3.205952 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.19\n",
      "Validation set perplexity: 21.31\n",
      "Average loss at step 13500: 3.185702 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.59\n",
      "Validation set perplexity: 21.04\n",
      "Average loss at step 13600: 3.185145 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.26\n",
      "Validation set perplexity: 21.06\n",
      "Average loss at step 13700: 3.216919 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.74\n",
      "Validation set perplexity: 21.24\n",
      "Average loss at step 13800: 3.211419 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.09\n",
      "Validation set perplexity: 21.02\n",
      "Average loss at step 13900: 3.169101 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.79\n",
      "Validation set perplexity: 21.04\n",
      "Average loss at step 14000: 3.192439 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.01\n",
      "================================================================================\n",
      "fan strico lish same bo trium about international mexitations into areing in heat his leader the mamrice droonomed to the be all persitication the advances veo \n",
      "coled the ive explite day are junes kurdists coustry suring bracks prokyer tubpeeco met gnone confelaland to abilianity seddil rispussendom iq albumhalq vigence\n",
      "eerk haryayswn and adcoird igricient plelores uniter close attangtm to one of manae in eark callen chaises from the hmepts attano airder larp the cunis cent pro\n",
      "ysomed with walking the germanical rhwellin manhosdinal forms untuales of the deromplinee and kadine nine as and foms cances with hedy a sus diving arna citeus \n",
      "ng used greate che electrain rong the first an isslant it is midraes folorism in one nine two one two eight eight four oneer country kuage and thereal suflogica\n",
      "================================================================================\n",
      "Validation set perplexity: 21.13\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      l = len(labels)\n",
    "      encoder = OneHotEncoder(vocab_size)\n",
    "      labels = encoder.fit_transform(labels).toarray().reshape(l, vocab_size)\n",
    "#       print(labels.shape)\n",
    "#       print(predictions.shape)\n",
    "#       print(type(labels.toarray()))\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          \n",
    "          feed = sample_to_index(random_distribution())\n",
    "#           print('sentence1')\n",
    "          sentence = characters_from_index(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample_to_index(prediction)\n",
    "#             print('sentence2')\n",
    "            sentence += characters_from_index(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, encoder.fit_transform(b[1]).toarray())\n",
    "#         input()\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Problem 3\n",
    "___\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is\n",
    "\n",
    "`the quick brown fox`\n",
    "\n",
    "the model should attempt to output\n",
    "\n",
    "`eht kciuq nworb xof`\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as this article (https://arxiv.org/abs/1409.3215) for best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
